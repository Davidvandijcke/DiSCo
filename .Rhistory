#####
# Load required packages
packages_load <- c("haven", "base", "data.table", "latex2exp", "CVXR",  # used to compute the weights using the alternative using mixtures of CDF
"here", "dplyr", "pracma", "quadprog")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(char = packages_load, character.only = TRUE)
#####
# Required functions
# loading the function for computing the optimal weights
source('DSC_weights_reg.R')
getwd()
# Set the working directory
setwd(file.path(here::here(), "R"))
#####
# Required functions
# loading the function for computing the optimal weights
source('DSC_weights_reg.R')
# load the data from the stata file
df <- read_dta(paste0("march_regready_1996.dta")) %>% setDT()
fwrite(df, "march_regready_1996.csv.gz")
head(df)
df <- df[, c("age", "educ", "contpov", "adj0contpov", "adj1contpov", "adj2contpov", "state_fips", "year", "hhseq")]
fwrite(df, "march_regready_1996.csv.gz")
# load the data from the stata file
df <- read_dta(paste0("march_regready_1996.dta")) %>% setDT()
df <- df[, c("age", "educ", "contpov", "adj0contpov", "adj1contpov", "adj2contpov", "state_fips", "year", "hhseq", "demgroup1")]
fwrite(df, "march_regready_1996.csv.gz")
# only considering individuals under the age of 65
df <- df[demgroup1 == 1]
# collapsing the data
df[,.(age=mean(age),
educ=mean(educ),
contpov=mean(contpov),
adj0contpov=mean(adj0contpov),
adj1contpov=mean(adj1contpov),
adj2contpov=mean(adj2contpov)),
by=c('state_fips', 'year', 'hhseq')]
results.over.years <- list()
#####
# Solving for the optimal weights in the DSC method and the alternative method using mixtures of
# distributions by year
# looping over the years from 1998 - 2014
for(yy in 1:7){
# obtaining the target state
target <- list(3)
target[[3]] <- df[state_fips==fips.target & year==(1997+yy)]$adj0contpov # for the first outcome,
# adj0contpov
v <- as.vector(c(rep(TRUE, floor(0.5*length(target[[3]]))),
rep(FALSE,ceiling(0.5*length(target[[3]])))))
set.seed(1860)
ind <- sample(v)
target[[1]] <- target[[3]][ind]
target[[2]] <- target[[3]][!ind]
# obtaining all control states
controlstates <- df[state_fips%in%control.states & year==(1997+yy)]
controls <- list()
helpcont <- unique(controlstates$state_fips)
for (ii in 1:length(helpcont)){
controls[[ii]] <- list(3)
controls[[ii]][[3]] <- controlstates[state_fips==helpcont[ii] & year==(1997+yy)]$adj0contpov
# for the first outcome,
# use adj0contpov
v <- as.vector(c(rep(TRUE, floor(0.5*length(controls[[ii]][[3]]))),
rep(FALSE,ceiling(0.5*length(controls[[ii]][[3]])))))
set.seed(1860)
ind <- sample(v)
controls[[ii]][[1]] <- controls[[ii]][[3]][ind]
controls[[ii]][[2]] <- controls[[ii]][[3]][!ind]
}
# generating the target
target.s <- mapply(myquant, seq(from=0, to=1, length.out=1001), MoreArgs =
list(X=target[[3]]))
# generating the controls
controls1 <- list(length(controls))
for (ii in 1:length(controls)){
controls1[[ii]] <- controls[[ii]][[3]]
}
# obtaining the optimal weights for the DSC method
DSC_res_weights <- DSC_weights_reg(controls1,as.vector(target[[3]]), 1000)
DSC_res2 <- DSC_bc(controls1,DSC_res_weights,seq(from=0,to=1,length.out=1001))
###### The mixture of distributions approach
# we again only focus on the first half of the data
# defining the grid on which we define the cumulative distribution functions
# obtaining the minimal and maximal values among all supports
# creating a list of controls with only the full data
grid.min <- min(c(min(target[[3]]),unlist(lapply(controls1,min))))
grid.max <- max(c(max(target[[3]]),unlist(lapply(controls1,max))))
# if grid.max-grid.min<=1 we round to the next 0.1.
grid.min <- floor(grid.min*10)/10
grid.max <- ceiling(grid.max*10)/10
# Estimating the empirical CDFs
CDF.control <- lapply(controls1,ecdf)
CDF.target <- ecdf(target[[3]])
# sampling uniformly on the grid
grid.rand <- runif(M,grid.min-0.25,grid.max+0.25)
#grid.rand <- runif(M,0,grid.max+0.25)
# ordered grid
grid.ord <- grid.rand[order(grid.rand)]
# getting the CDF from the quantile function
DSC_res2.cdfF <- ecdf(DSC_res2[[2]])
DSC_res2.cdf <- DSC_res2.cdfF(grid.ord)
# Evaluating the CDF on the random grid
CDF.matrix <- matrix(0,nrow=length(grid.rand), ncol = (length(controls1)+1))
CDF.matrix[,1] <- CDF.target(grid.rand)
for (ii in 1:length(controls1)){
CDF.matrix[,(ii+1)] <- CDF.control[[ii]](grid.rand)
}
# Solving the convex problem with CVXR
# the variable we are after
theweights <- Variable(length(controls1))
# the objective function
objective <- cvxr_norm((CDF.matrix[,2:ncol(CDF.matrix)] %*% theweights - CDF.matrix[,1]))
# the constraints for the unit simplex
constraints <- list(theweights>=0, sum_entries(theweights) == 1)
# the optimization problem
problem <- Problem(Minimize(objective),constraints)
# solving the optimization problem
results <- solve(problem, solver = "SCS")
# returning the optimal weights and the value function which provides the
# squared Wasserstein distance between the target and the corresponding barycenter
theweights.opt <- results$getValue(theweights)
thedistance.opt <- results$value*1/M*(grid.max - grid.min)
themean <- CDF.matrix[,2:ncol(CDF.matrix)]%*%theweights.opt
themean.order <- themean[order(grid.rand, decreasing=FALSE)]
target.order <- CDF.matrix[order(grid.rand, decreasing=FALSE),1]
results.over.years[[yy]] <- list()
results.over.years[[yy]][[1]] <- list(DSC_res_weights, DSC_res2, DSC_res2.cdf) # DSC estimator
results.over.years[[yy]][[2]] <- list(theweights.opt, themean, themean.order) # the mixture
results.over.years[[yy]][[3]] <- list(target.s, target.order, grid.ord, as.vector(target[[3]]))
results.over.years[[yy]][[4]] <- list(controls1, CDF.matrix)
}
#####
# Values to be chosen by the researcher
M <- 1000 # draws for samples to compute the respective integrals.
fips.target <- 2 # target state is AK
# collapsing the data
df[,.(age=mean(age),
educ=mean(educ),
contpov=mean(contpov),
adj0contpov=mean(adj0contpov),
adj1contpov=mean(adj1contpov),
adj2contpov=mean(adj2contpov)),
by=c('state_fips', 'year', 'hhseq')]
results.over.years <- list()
#####
# Solving for the optimal weights in the DSC method and the alternative method using mixtures of
# distributions by year
# looping over the years from 1998 - 2014
for(yy in 1:7){
# obtaining the target state
target <- list(3)
target[[3]] <- df[state_fips==fips.target & year==(1997+yy)]$adj0contpov # for the first outcome,
# adj0contpov
v <- as.vector(c(rep(TRUE, floor(0.5*length(target[[3]]))),
rep(FALSE,ceiling(0.5*length(target[[3]])))))
set.seed(1860)
ind <- sample(v)
target[[1]] <- target[[3]][ind]
target[[2]] <- target[[3]][!ind]
# obtaining all control states
controlstates <- df[state_fips%in%control.states & year==(1997+yy)]
controls <- list()
helpcont <- unique(controlstates$state_fips)
for (ii in 1:length(helpcont)){
controls[[ii]] <- list(3)
controls[[ii]][[3]] <- controlstates[state_fips==helpcont[ii] & year==(1997+yy)]$adj0contpov
# for the first outcome,
# use adj0contpov
v <- as.vector(c(rep(TRUE, floor(0.5*length(controls[[ii]][[3]]))),
rep(FALSE,ceiling(0.5*length(controls[[ii]][[3]])))))
set.seed(1860)
ind <- sample(v)
controls[[ii]][[1]] <- controls[[ii]][[3]][ind]
controls[[ii]][[2]] <- controls[[ii]][[3]][!ind]
}
# generating the target
target.s <- mapply(myquant, seq(from=0, to=1, length.out=1001), MoreArgs =
list(X=target[[3]]))
# generating the controls
controls1 <- list(length(controls))
for (ii in 1:length(controls)){
controls1[[ii]] <- controls[[ii]][[3]]
}
# obtaining the optimal weights for the DSC method
DSC_res_weights <- DSC_weights_reg(controls1,as.vector(target[[3]]), 1000)
DSC_res2 <- DSC_bc(controls1,DSC_res_weights,seq(from=0,to=1,length.out=1001))
###### The mixture of distributions approach
# we again only focus on the first half of the data
# defining the grid on which we define the cumulative distribution functions
# obtaining the minimal and maximal values among all supports
# creating a list of controls with only the full data
grid.min <- min(c(min(target[[3]]),unlist(lapply(controls1,min))))
grid.max <- max(c(max(target[[3]]),unlist(lapply(controls1,max))))
# if grid.max-grid.min<=1 we round to the next 0.1.
grid.min <- floor(grid.min*10)/10
grid.max <- ceiling(grid.max*10)/10
# Estimating the empirical CDFs
CDF.control <- lapply(controls1,ecdf)
CDF.target <- ecdf(target[[3]])
# sampling uniformly on the grid
grid.rand <- runif(M,grid.min-0.25,grid.max+0.25)
#grid.rand <- runif(M,0,grid.max+0.25)
# ordered grid
grid.ord <- grid.rand[order(grid.rand)]
# getting the CDF from the quantile function
DSC_res2.cdfF <- ecdf(DSC_res2[[2]])
DSC_res2.cdf <- DSC_res2.cdfF(grid.ord)
# Evaluating the CDF on the random grid
CDF.matrix <- matrix(0,nrow=length(grid.rand), ncol = (length(controls1)+1))
CDF.matrix[,1] <- CDF.target(grid.rand)
for (ii in 1:length(controls1)){
CDF.matrix[,(ii+1)] <- CDF.control[[ii]](grid.rand)
}
# Solving the convex problem with CVXR
# the variable we are after
theweights <- Variable(length(controls1))
# the objective function
objective <- cvxr_norm((CDF.matrix[,2:ncol(CDF.matrix)] %*% theweights - CDF.matrix[,1]))
# the constraints for the unit simplex
constraints <- list(theweights>=0, sum_entries(theweights) == 1)
# the optimization problem
problem <- Problem(Minimize(objective),constraints)
# solving the optimization problem
results <- solve(problem, solver = "SCS")
# returning the optimal weights and the value function which provides the
# squared Wasserstein distance between the target and the corresponding barycenter
theweights.opt <- results$getValue(theweights)
thedistance.opt <- results$value*1/M*(grid.max - grid.min)
themean <- CDF.matrix[,2:ncol(CDF.matrix)]%*%theweights.opt
themean.order <- themean[order(grid.rand, decreasing=FALSE)]
target.order <- CDF.matrix[order(grid.rand, decreasing=FALSE),1]
results.over.years[[yy]] <- list()
results.over.years[[yy]][[1]] <- list(DSC_res_weights, DSC_res2, DSC_res2.cdf) # DSC estimator
results.over.years[[yy]][[2]] <- list(theweights.opt, themean, themean.order) # the mixture
results.over.years[[yy]][[3]] <- list(target.s, target.order, grid.ord, as.vector(target[[3]]))
results.over.years[[yy]][[4]] <- list(controls1, CDF.matrix)
}
